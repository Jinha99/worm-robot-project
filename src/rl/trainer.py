"""
Worm Robot Simulation - RL Trainer
DQN í•™ìŠµ ë£¨í”„ êµ¬í˜„ (ê°„ë‹¨í•œ ë²„ì „)
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from pypdevs.simulator import Simulator
from rl.replay_buffer import ReplayBuffer
from config import STATUS_RUNNING, STATUS_WIN, STATUS_PARTIAL_WIN, STATUS_FAIL

try:
    from torch.utils.tensorboard import SummaryWriter  # type: ignore
    TENSORBOARD_AVAILABLE = True
except ImportError:
    TENSORBOARD_AVAILABLE = False
    print("âš ï¸  TensorBoardë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ ì„¤ì¹˜í•˜ì„¸ìš”: pip3 install tensorboard")


class DQNTrainer:
    """
    DQN í•™ìŠµ ë£¨í”„ë¥¼ ê´€ë¦¬í•˜ëŠ” í´ë˜ìŠ¤
    
    ì£¼ì˜: í˜„ì¬ëŠ” ì—í”¼ì†Œë“œ ì „ì²´ë¥¼ ì‹¤í–‰ í›„ ë³´ìƒ ê³„ì‚°í•˜ëŠ” ê°„ë‹¨í•œ ë²„ì „
    """

    def __init__(
        self,
        agent,
        create_system_fn,
        num_episodes=1000,
        termination_time=100,
        batch_size=32,
        buffer_size=10000,
        log_interval=10,
        save_interval=100,
        model_path="models/dqn_worm_robot.pth",
        use_tensorboard=True,
        tensorboard_dir="runs/worm_robot_dqn"
    ):
        """
        Args:
            agent: DQN ì—ì´ì „íŠ¸
            create_system_fn: WormRobotSystemì„ ìƒì„±í•˜ëŠ” í•¨ìˆ˜
            num_episodes: í•™ìŠµ ì—í”¼ì†Œë“œ ìˆ˜
            termination_time: ì‹œë®¬ë ˆì´ì…˜ ìµœëŒ€ ì‹œê°„ (ì´ˆ)
            batch_size: ë°°ì¹˜ í¬ê¸°
            buffer_size: Replay Buffer í¬ê¸°
            log_interval: ë¡œê·¸ ì¶œë ¥ ê°„ê²©
            save_interval: ëª¨ë¸ ì €ì¥ ê°„ê²©
            model_path: ëª¨ë¸ ì €ì¥ ê²½ë¡œ
            use_tensorboard: TensorBoard ì‚¬ìš© ì—¬ë¶€
            tensorboard_dir: TensorBoard ë¡œê·¸ ë””ë ‰í† ë¦¬
        """
        self.agent = agent
        self.create_system_fn = create_system_fn
        self.num_episodes = num_episodes
        self.termination_time = termination_time
        self.batch_size = batch_size
        self.log_interval = log_interval
        self.save_interval = save_interval
        self.model_path = model_path
        
        # Replay Buffer
        self.replay_buffer = ReplayBuffer(capacity=buffer_size)
        
        # TensorBoard
        self.writer = None
        if use_tensorboard and TENSORBOARD_AVAILABLE:
            self.writer = SummaryWriter(tensorboard_dir)
            print(f"ğŸ“Š TensorBoard ë¡œê¹… í™œì„±í™”: {tensorboard_dir}")
            print(f"   ì‹¤í–‰: tensorboard --logdir=runs")
        
        # í†µê³„
        self.stats = {
            "episode_rewards": [],
            "episode_steps": [],
            "episode_losses": [],
            "success_count": 0,
            "partial_success_count": 0,
            "fail_count": 0
        }

    def train(self):
        """í•™ìŠµ ë£¨í”„ ì‹¤í–‰"""
        print("=" * 60)
        print("DQN í•™ìŠµ ì‹œì‘")
        print("=" * 60)
        print(f"ì—í”¼ì†Œë“œ ìˆ˜: {self.num_episodes}")
        print(f"ì‹œë®¬ë ˆì´ì…˜ ì‹œê°„: {self.termination_time}ì´ˆ")
        print(f"ë°°ì¹˜ í¬ê¸°: {self.batch_size}")
        print(f"ì´ˆê¸° Epsilon: {self.agent.epsilon:.3f}")
        print("=" * 60)
        
        for episode in range(self.num_episodes):
            episode_reward, episode_steps, episode_status, num_experiences = self._run_episode()

            # í†µê³„ ì—…ë°ì´íŠ¸
            self.stats["episode_rewards"].append(episode_reward)
            self.stats["episode_steps"].append(episode_steps)

            if episode_status == STATUS_WIN:
                self.stats["success_count"] += 1
            elif episode_status == STATUS_PARTIAL_WIN:
                self.stats["partial_success_count"] += 1
            elif episode_status == STATUS_FAIL:
                self.stats["fail_count"] += 1
            
            # í•™ìŠµ (ë°°ì¹˜ê°€ ì¶©ë¶„íˆ ìŒ“ì´ë©´)
            if len(self.replay_buffer) >= self.batch_size:
                total_loss = 0.0
                # ì—¬ëŸ¬ ë²ˆ í•™ìŠµ
                for _ in range(5):
                    # ReplayBuffer.sample()ì€ (states, actions, rewards, next_states, dones) ë°˜í™˜
                    # agent.train()ì€ [(s,a,r,s',d), ...] í˜•íƒœ ê¸°ëŒ€
                    states, actions, rewards, next_states, dones = self.replay_buffer.sample(self.batch_size)
                    batch = list(zip(states, actions, rewards, next_states, dones))
                    loss = self.agent.train(batch)
                    total_loss += loss
                self.stats["episode_losses"].append(total_loss / 5)
            else:
                self.stats["episode_losses"].append(0.0)
            
            # Epsilon ê°ì†Œ
            self.agent.update_epsilon()
            
            # TensorBoard ë¡œê¹…
            if self.writer is not None:
                self.writer.add_scalar('Reward/episode', episode_reward, episode)
                self.writer.add_scalar('Steps/episode', episode_steps, episode)
                self.writer.add_scalar('Loss/episode', self.stats["episode_losses"][-1], episode)
                self.writer.add_scalar('Epsilon', self.agent.epsilon, episode)
                self.writer.add_scalar('Success/total', self.stats["success_count"], episode)
                self.writer.add_scalar('Success/partial', self.stats["partial_success_count"], episode)
                self.writer.add_scalar('Fail/total', self.stats["fail_count"], episode)
                self.writer.add_scalar('Experiences/episode', num_experiences, episode)

                # ì„±ê³µ/ë¶€ë¶„ì„±ê³µ/ì‹¤íŒ¨ë¥¼ 0 ë˜ëŠ” 1ë¡œ ê¸°ë¡
                if episode_status == STATUS_WIN:
                    self.writer.add_scalar('Result/win', 1, episode)
                    self.writer.add_scalar('Result/partial', 0, episode)
                    self.writer.add_scalar('Result/fail', 0, episode)
                elif episode_status == STATUS_PARTIAL_WIN:
                    self.writer.add_scalar('Result/win', 0, episode)
                    self.writer.add_scalar('Result/partial', 1, episode)
                    self.writer.add_scalar('Result/fail', 0, episode)
                elif episode_status == STATUS_FAIL:
                    self.writer.add_scalar('Result/win', 0, episode)
                    self.writer.add_scalar('Result/partial', 0, episode)
                    self.writer.add_scalar('Result/fail', 1, episode)
            
            # ë¡œê·¸ ì¶œë ¥
            if (episode + 1) % self.log_interval == 0:
                recent = self.log_interval
                avg_reward = sum(self.stats["episode_rewards"][-recent:]) / recent
                avg_steps = sum(self.stats["episode_steps"][-recent:]) / recent
                avg_loss = sum(self.stats["episode_losses"][-recent:]) / recent
                
                print(
                    f"Ep {episode + 1:4d}/{self.num_episodes} | "
                    f"Reward: {avg_reward:6.1f} | "
                    f"Steps: {avg_steps:4.1f} | "
                    f"Loss: {avg_loss:.4f} | "
                    f"Îµ: {self.agent.epsilon:.3f} | "
                    f"Win: {self.stats['success_count']:3d} | "
                    f"Partial: {self.stats['partial_success_count']:3d} | "
                    f"Fail: {self.stats['fail_count']:3d}"
                )
            
            # ëª¨ë¸ ì €ì¥
            if (episode + 1) % self.save_interval == 0:
                self._save_model()
        
        # ìµœì¢… ëª¨ë¸ ì €ì¥
        self._save_model()
        
        # TensorBoard writer ì¢…ë£Œ
        if self.writer is not None:
            self.writer.close()
            print("\nğŸ“Š TensorBoard ë¡œê·¸ ì €ì¥ ì™„ë£Œ")
        
        print("\n" + "=" * 60)
        print("í•™ìŠµ ì™„ë£Œ!")
        print(f"ì´ ì™„ì „ ì„±ê³µ: {self.stats['success_count']}")
        print(f"ì´ ë¶€ë¶„ ì„±ê³µ: {self.stats['partial_success_count']}")
        print(f"ì´ ì‹¤íŒ¨: {self.stats['fail_count']}")
        print(f"ì™„ì „ ì„±ê³µë¥ : {self.stats['success_count'] / self.num_episodes * 100:.1f}%")
        print(f"ë¶€ë¶„ ì„±ê³µë¥ : {self.stats['partial_success_count'] / self.num_episodes * 100:.1f}%")
        combined_success = self.stats['success_count'] + self.stats['partial_success_count']
        print(f"ì „ì²´ ì„±ê³µë¥  (ë¶€ë¶„+ì™„ì „): {combined_success / self.num_episodes * 100:.1f}%")
        print("=" * 60)
        
        return self.stats

    def _run_episode(self):
        """
        ë‹¨ì¼ ì—í”¼ì†Œë“œ ì‹¤í–‰ (ìŠ¤í…ë³„ ê²½í—˜ ìˆ˜ì§‘ ë²„ì „)

        Returns:
            tuple: (total_reward, step_count, final_status, num_experiences)
        """
        # ìƒˆë¡œìš´ ì‹œìŠ¤í…œ ìƒì„± (ëœë¤ ì´ˆê¸°í™”)
        system = self.create_system_fn(rl_agent=self.agent)

        # ì‹œë®¬ë ˆì´í„° ì„¤ì • ë° ì‹¤í–‰
        sim = Simulator(system)
        sim.setClassicDEVS()
        sim.setTerminationTime(self.termination_time)
        sim.simulate()

        # ìµœì¢… ìƒíƒœ ìˆ˜ì§‘
        final_status = system.environment.state.status
        step_count = system.environment.state.step_count

        # Controllerì—ì„œ ìŠ¤í…ë³„ ê²½í—˜ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        experiences = system.controller.get_step_experiences()

        # Replay Bufferì— ê²½í—˜ ì¶”ê°€
        for exp in experiences:
            state, action, reward, next_state, done = exp
            self.replay_buffer.add(state, action, reward, next_state, float(done))

        # ì—í”¼ì†Œë“œ ì´ ë³´ìƒ ê³„ì‚° (ëª¨ë“  ê²½í—˜ì˜ ë³´ìƒ í•©ê³„)
        total_reward = sum(exp[2] for exp in experiences) if experiences else 0.0

        # ì—í”¼ì†Œë“œ ì¢…ë£Œ ì‹œ ì¶”ê°€ ë³´ìƒ (ì „ì²´ ê²°ê³¼ì— ë”°ë¼)
        if final_status == STATUS_WIN:
            # ì™„ì „ ì„±ê³µ ë³´ë„ˆìŠ¤
            total_reward += 500.0
        elif final_status == STATUS_PARTIAL_WIN:
            # ë¶€ë¶„ ì„±ê³µ ë³´ë„ˆìŠ¤
            total_reward += 200.0
        elif final_status == STATUS_FAIL:
            # ì‹¤íŒ¨ í˜ë„í‹°
            total_reward -= 100.0

        return total_reward, step_count, final_status, len(experiences)

    def _save_model(self):
        """ëª¨ë¸ ì €ì¥"""
        os.makedirs(os.path.dirname(self.model_path) if os.path.dirname(self.model_path) else "models", exist_ok=True)
        self.agent.save(self.model_path)

    def evaluate(self, num_episodes=10):
        """í•™ìŠµëœ ì—ì´ì „íŠ¸ í‰ê°€"""
        print("\n" + "=" * 60)
        print(f"í‰ê°€ ì‹œì‘ ({num_episodes} ì—í”¼ì†Œë“œ)")
        print("=" * 60)
        
        success_count = 0
        total_rewards = []
        total_steps = []
        
        # ì›ë˜ epsilon ì €ì¥
        original_epsilon = self.agent.epsilon
        self.agent.epsilon = 0.0  # í‰ê°€ ì‹œì—ëŠ” íƒí—˜ ì•ˆ í•¨
        
        for episode in range(num_episodes):
            reward, steps, status, _ = self._run_episode()
            total_rewards.append(reward)
            total_steps.append(steps)

            if status == STATUS_WIN:
                success_count += 1
        
        # Epsilon ë³µì›
        self.agent.epsilon = original_epsilon
        
        avg_reward = sum(total_rewards) / num_episodes
        avg_steps = sum(total_steps) / num_episodes
        success_rate = success_count / num_episodes * 100
        
        print(f"í‰ê·  ë³´ìƒ: {avg_reward:.2f}")
        print(f"í‰ê·  ìŠ¤í…: {avg_steps:.1f}")
        print(f"ì„±ê³µë¥ : {success_rate:.1f}%")
        print("=" * 60)
        
        return {
            "success_rate": success_rate,
            "avg_reward": avg_reward,
            "avg_steps": avg_steps
        }

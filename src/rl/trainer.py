"""
Worm Robot Simulation - RL Trainer
DQN í•™ìŠµ ë£¨í”„ êµ¬í˜„ (ê°„ë‹¨í•œ ë²„ì „)
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from pypdevs.simulator import Simulator
from rl.replay_buffer import ReplayBuffer
from config import STATUS_RUNNING, STATUS_WIN, STATUS_PARTIAL_WIN, STATUS_FAIL

try:
    from torch.utils.tensorboard import SummaryWriter  # type: ignore
    TENSORBOARD_AVAILABLE = True
except ImportError:
    TENSORBOARD_AVAILABLE = False
    print("âš ï¸  TensorBoardë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ ì„¤ì¹˜í•˜ì„¸ìš”: pip3 install tensorboard")


class DQNTrainer:
    """
    DQN í•™ìŠµ ë£¨í”„ë¥¼ ê´€ë¦¬í•˜ëŠ” í´ë˜ìŠ¤

    ì£¼ì˜: í˜„ì¬ëŠ” ì—í”¼ì†Œë“œ ì „ì²´ë¥¼ ì‹¤í–‰ í›„ ë³´ìƒ ê³„ì‚°í•˜ëŠ” ê°„ë‹¨í•œ ë²„ì „
    ì»¤ë¦¬í˜ëŸ¼ í•™ìŠµ ì§€ì› (ë‹¨ê³„ì  ë‚œì´ë„ ì¦ê°€)
    """

    def __init__(
        self,
        agent,
        create_system_fn,
        num_episodes=1000,
        termination_time=100,
        batch_size=32,
        buffer_size=10000,
        log_interval=10,
        save_interval=100,
        model_path="models/dqn_worm_robot.pth",
        use_tensorboard=True,
        tensorboard_dir="runs/worm_robot_dqn",
        curriculum_stages=None,
        progression_threshold=0.7,
        progression_window=100
    ):
        """
        Args:
            agent: DQN ì—ì´ì „íŠ¸
            create_system_fn: WormRobotSystemì„ ìƒì„±í•˜ëŠ” í•¨ìˆ˜
            num_episodes: í•™ìŠµ ì—í”¼ì†Œë“œ ìˆ˜
            termination_time: ì‹œë®¬ë ˆì´ì…˜ ìµœëŒ€ ì‹œê°„ (ì´ˆ)
            batch_size: ë°°ì¹˜ í¬ê¸°
            buffer_size: Replay Buffer í¬ê¸°
            log_interval: ë¡œê·¸ ì¶œë ¥ ê°„ê²©
            save_interval: ëª¨ë¸ ì €ì¥ ê°„ê²©
            model_path: ëª¨ë¸ ì €ì¥ ê²½ë¡œ
            use_tensorboard: TensorBoard ì‚¬ìš© ì—¬ë¶€
            tensorboard_dir: TensorBoard ë¡œê·¸ ë””ë ‰í† ë¦¬
            curriculum_stages: ì»¤ë¦¬í˜ëŸ¼ í•™ìŠµ ë‹¨ê³„ ë¦¬ìŠ¤íŠ¸ (Noneì´ë©´ ë‹¨ì¼ ë‹¨ê³„)
                               ì˜ˆ: [{"name": "Stage1", "num_robots": 1, "min_distance": 0},
                                    {"name": "Stage2", "num_robots": 2, "min_distance": 6}]
            progression_threshold: ë‹¤ìŒ ë‹¨ê³„ë¡œ ì§„í–‰í•˜ê¸° ìœ„í•œ ì„±ê³µë¥  ì„ê³„ê°’ (0.0~1.0)
            progression_window: ì„±ê³µë¥  ê³„ì‚°ì— ì‚¬ìš©í•  ìµœê·¼ ì—í”¼ì†Œë“œ ìˆ˜
        """
        self.agent = agent
        self.create_system_fn = create_system_fn
        self.num_episodes = num_episodes
        self.termination_time = termination_time
        self.batch_size = batch_size
        self.log_interval = log_interval
        self.save_interval = save_interval
        self.model_path = model_path

        # Replay Buffer
        self.replay_buffer = ReplayBuffer(capacity=buffer_size)

        # ì»¤ë¦¬í˜ëŸ¼ í•™ìŠµ ì„¤ì •
        self.curriculum_stages = curriculum_stages
        self.progression_threshold = progression_threshold
        self.progression_window = progression_window
        self.current_stage_idx = 0
        self.stage_start_episode = 0

        # TensorBoard
        self.writer = None
        if use_tensorboard and TENSORBOARD_AVAILABLE:
            self.writer = SummaryWriter(tensorboard_dir)
            print(f"ğŸ“Š TensorBoard ë¡œê¹… í™œì„±í™”: {tensorboard_dir}")
            print(f"   ì‹¤í–‰: tensorboard --logdir=runs")

        # í†µê³„
        self.stats = {
            "episode_rewards": [],
            "episode_steps": [],
            "episode_losses": [],
            "episode_results": [],  # ê° ì—í”¼ì†Œë“œ ê²°ê³¼ (STATUS_WIN, STATUS_PARTIAL_WIN, etc.)
            "success_count": 0,
            "partial_success_count": 0,
            "fail_count": 0,
            "timeout_count": 0  # ì‹œê°„ ì´ˆê³¼ ì¹´ìš´íŠ¸ ì¶”ê°€
        }

    def train(self):
        """í•™ìŠµ ë£¨í”„ ì‹¤í–‰ (ì»¤ë¦¬í˜ëŸ¼ í•™ìŠµ ì§€ì›)"""
        print("=" * 60)
        print("DQN í•™ìŠµ ì‹œì‘")
        print("=" * 60)
        print(f"ì—í”¼ì†Œë“œ ìˆ˜: {self.num_episodes}")
        print(f"ì‹œë®¬ë ˆì´ì…˜ ì‹œê°„: {self.termination_time}ì´ˆ")
        print(f"ë°°ì¹˜ í¬ê¸°: {self.batch_size}")
        print(f"ì´ˆê¸° Epsilon: {self.agent.epsilon:.3f}")

        # ì»¤ë¦¬í˜ëŸ¼ í•™ìŠµ ì •ë³´ ì¶œë ¥
        if self.curriculum_stages:
            print(f"\nğŸ“š ì»¤ë¦¬í˜ëŸ¼ í•™ìŠµ í™œì„±í™”:")
            for i, stage in enumerate(self.curriculum_stages):
                print(f"   {i+1}. {stage['name']}: {stage['num_robots']}ê°œ ë¡œë´‡, ìµœì†Œê±°ë¦¬ {stage['min_distance']}")
            print(f"   ì§„í–‰ ì¡°ê±´: ì„±ê³µë¥  {self.progression_threshold*100:.0f}% (ìµœê·¼ {self.progression_window} ì—í”¼ì†Œë“œ)")

            # ì²« ë²ˆì§¸ ë‹¨ê³„ ì„¤ì •
            import config
            first_stage = self.curriculum_stages[0]
            config.NUM_ROBOTS = first_stage["num_robots"]
            config.MIN_ROBOT_DISTANCE = first_stage["min_distance"]
            print(f"\nğŸš€ ì‹œì‘ ë‹¨ê³„: {first_stage['name']}")

        print("=" * 60)
        
        for episode in range(self.num_episodes):
            episode_reward, episode_steps, episode_status, num_experiences = self._run_episode()

            # í†µê³„ ì—…ë°ì´íŠ¸
            self.stats["episode_rewards"].append(episode_reward)
            self.stats["episode_steps"].append(episode_steps)
            self.stats["episode_results"].append(episode_status)  # ì—í”¼ì†Œë“œ ê²°ê³¼ ì €ì¥

            if episode_status == STATUS_WIN:
                self.stats["success_count"] += 1
            elif episode_status == STATUS_PARTIAL_WIN:
                self.stats["partial_success_count"] += 1
            elif episode_status == STATUS_FAIL:
                self.stats["fail_count"] += 1
            else:  # STATUS_RUNNING (ì‹œê°„ ì´ˆê³¼)
                self.stats["timeout_count"] += 1
            
            # í•™ìŠµ (ë°°ì¹˜ê°€ ì¶©ë¶„íˆ ìŒ“ì´ë©´)
            if len(self.replay_buffer) >= self.batch_size:
                total_loss = 0.0
                # ì—¬ëŸ¬ ë²ˆ í•™ìŠµ
                for _ in range(5):
                    # ReplayBuffer.sample()ì€ (states, actions, rewards, next_states, dones) ë°˜í™˜
                    # agent.train()ì€ [(s,a,r,s',d), ...] í˜•íƒœ ê¸°ëŒ€
                    states, actions, rewards, next_states, dones = self.replay_buffer.sample(self.batch_size)
                    batch = list(zip(states, actions, rewards, next_states, dones))
                    loss = self.agent.train(batch)
                    total_loss += loss
                self.stats["episode_losses"].append(total_loss / 5)
            else:
                self.stats["episode_losses"].append(0.0)
            
            # Epsilon ê°ì†Œ
            self.agent.update_epsilon()

            # ì»¤ë¦¬í˜ëŸ¼ ë‹¨ê³„ ì§„í–‰ ì²´í¬
            if self._check_stage_progression(episode):
                self._progress_to_next_stage(episode)

            # TensorBoard ë¡œê¹…
            if self.writer is not None:
                self.writer.add_scalar('Reward/episode', episode_reward, episode)
                self.writer.add_scalar('Steps/episode', episode_steps, episode)
                self.writer.add_scalar('Loss/episode', self.stats["episode_losses"][-1], episode)
                self.writer.add_scalar('Epsilon', self.agent.epsilon, episode)
                self.writer.add_scalar('Success/total', self.stats["success_count"], episode)
                self.writer.add_scalar('Success/partial', self.stats["partial_success_count"], episode)
                self.writer.add_scalar('Fail/total', self.stats["fail_count"], episode)
                self.writer.add_scalar('Timeout/total', self.stats["timeout_count"], episode)
                self.writer.add_scalar('Experiences/episode', num_experiences, episode)

                # ì„±ê³µ/ë¶€ë¶„ì„±ê³µ/ì‹¤íŒ¨/ì‹œê°„ì´ˆê³¼ë¥¼ 0 ë˜ëŠ” 1ë¡œ ê¸°ë¡
                if episode_status == STATUS_WIN:
                    self.writer.add_scalar('Result/win', 1, episode)
                    self.writer.add_scalar('Result/partial', 0, episode)
                    self.writer.add_scalar('Result/fail', 0, episode)
                    self.writer.add_scalar('Result/timeout', 0, episode)
                elif episode_status == STATUS_PARTIAL_WIN:
                    self.writer.add_scalar('Result/win', 0, episode)
                    self.writer.add_scalar('Result/partial', 1, episode)
                    self.writer.add_scalar('Result/fail', 0, episode)
                    self.writer.add_scalar('Result/timeout', 0, episode)
                elif episode_status == STATUS_FAIL:
                    self.writer.add_scalar('Result/win', 0, episode)
                    self.writer.add_scalar('Result/partial', 0, episode)
                    self.writer.add_scalar('Result/fail', 1, episode)
                    self.writer.add_scalar('Result/timeout', 0, episode)
                else:  # STATUS_RUNNING (ì‹œê°„ ì´ˆê³¼)
                    self.writer.add_scalar('Result/win', 0, episode)
                    self.writer.add_scalar('Result/partial', 0, episode)
                    self.writer.add_scalar('Result/fail', 0, episode)
                    self.writer.add_scalar('Result/timeout', 1, episode)
            
            # ë¡œê·¸ ì¶œë ¥
            if (episode + 1) % self.log_interval == 0:
                recent = self.log_interval
                avg_reward = sum(self.stats["episode_rewards"][-recent:]) / recent
                avg_steps = sum(self.stats["episode_steps"][-recent:]) / recent
                avg_loss = sum(self.stats["episode_losses"][-recent:]) / recent
                
                print(
                    f"Ep {episode + 1:4d}/{self.num_episodes} | "
                    f"Reward: {avg_reward:6.1f} | "
                    f"Steps: {avg_steps:4.1f} | "
                    f"Loss: {avg_loss:.4f} | "
                    f"Îµ: {self.agent.epsilon:.3f} | "
                    f"W: {self.stats['success_count']:3d} | "
                    f"P: {self.stats['partial_success_count']:3d} | "
                    f"F: {self.stats['fail_count']:3d} | "
                    f"T: {self.stats['timeout_count']:3d}"
                )
            
            # ëª¨ë¸ ì €ì¥
            if (episode + 1) % self.save_interval == 0:
                self._save_model()
        
        # ìµœì¢… ëª¨ë¸ ì €ì¥
        self._save_model()
        
        # TensorBoard writer ì¢…ë£Œ
        if self.writer is not None:
            self.writer.close()
            print("\nğŸ“Š TensorBoard ë¡œê·¸ ì €ì¥ ì™„ë£Œ")
        
        print("\n" + "=" * 60)
        print("í•™ìŠµ ì™„ë£Œ!")
        print(f"ì´ ì™„ì „ ì„±ê³µ: {self.stats['success_count']} ({self.stats['success_count'] / self.num_episodes * 100:.1f}%)")
        print(f"ì´ ë¶€ë¶„ ì„±ê³µ: {self.stats['partial_success_count']} ({self.stats['partial_success_count'] / self.num_episodes * 100:.1f}%)")
        print(f"ì´ ì¶©ëŒ ì‹¤íŒ¨: {self.stats['fail_count']} ({self.stats['fail_count'] / self.num_episodes * 100:.1f}%)")
        print(f"ì´ ì‹œê°„ ì´ˆê³¼: {self.stats['timeout_count']} ({self.stats['timeout_count'] / self.num_episodes * 100:.1f}%)")
        combined_success = self.stats['success_count'] + self.stats['partial_success_count']
        print(f"ì „ì²´ ì„±ê³µë¥ : {combined_success / self.num_episodes * 100:.1f}%")
        print("=" * 60)
        
        return self.stats

    def _run_episode(self):
        """
        ë‹¨ì¼ ì—í”¼ì†Œë“œ ì‹¤í–‰ (ìŠ¤í…ë³„ ê²½í—˜ ìˆ˜ì§‘ ë²„ì „)

        Returns:
            tuple: (total_reward, step_count, final_status, num_experiences)
        """
        # ìƒˆë¡œìš´ ì‹œìŠ¤í…œ ìƒì„± (ëœë¤ ì´ˆê¸°í™”)
        system = self.create_system_fn(rl_agent=self.agent)

        # ì‹œë®¬ë ˆì´í„° ì„¤ì • ë° ì‹¤í–‰
        sim = Simulator(system)
        sim.setClassicDEVS()
        sim.setTerminationTime(self.termination_time)
        sim.simulate()

        # ìµœì¢… ìƒíƒœ ìˆ˜ì§‘
        final_status = system.environment.state.status
        step_count = system.environment.state.step_count

        # Controllerì—ì„œ ìŠ¤í…ë³„ ê²½í—˜ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        experiences = system.controller.get_step_experiences()

        # Replay Bufferì— ê²½í—˜ ì¶”ê°€
        for exp in experiences:
            state, action, reward, next_state, done = exp
            self.replay_buffer.add(state, action, reward, next_state, float(done))

        # ì—í”¼ì†Œë“œ ì´ ë³´ìƒ ê³„ì‚° (ëª¨ë“  ê²½í—˜ì˜ ë³´ìƒ í•©ê³„)
        total_reward = sum(exp[2] for exp in experiences) if experiences else 0.0

        # ì—í”¼ì†Œë“œ ì¢…ë£Œ ì‹œ ì¶”ê°€ ë³´ìƒ (ì „ì²´ ê²°ê³¼ì— ë”°ë¼)
        if final_status == STATUS_WIN:
            # ì™„ì „ ì„±ê³µ ë³´ë„ˆìŠ¤
            total_reward += 500.0
        elif final_status == STATUS_PARTIAL_WIN:
            # ë¶€ë¶„ ì„±ê³µ ë³´ë„ˆìŠ¤
            total_reward += 200.0
        elif final_status == STATUS_FAIL:
            # ì‹¤íŒ¨ í˜ë„í‹°
            total_reward -= 100.0

        return total_reward, step_count, final_status, len(experiences)

    def _save_model(self):
        """ëª¨ë¸ ì €ì¥"""
        os.makedirs(os.path.dirname(self.model_path) if os.path.dirname(self.model_path) else "models", exist_ok=True)
        self.agent.save(self.model_path)

    def evaluate(self, num_episodes=10):
        """í•™ìŠµëœ ì—ì´ì „íŠ¸ í‰ê°€"""
        print("\n" + "=" * 60)
        print(f"í‰ê°€ ì‹œì‘ ({num_episodes} ì—í”¼ì†Œë“œ)")
        print("=" * 60)

        success_count = 0
        total_rewards = []
        total_steps = []

        # ì›ë˜ epsilon ì €ì¥
        original_epsilon = self.agent.epsilon
        self.agent.epsilon = 0.0  # í‰ê°€ ì‹œì—ëŠ” íƒí—˜ ì•ˆ í•¨

        for episode in range(num_episodes):
            reward, steps, status, _ = self._run_episode()
            total_rewards.append(reward)
            total_steps.append(steps)

            if status == STATUS_WIN:
                success_count += 1

        # Epsilon ë³µì›
        self.agent.epsilon = original_epsilon

        avg_reward = sum(total_rewards) / num_episodes
        avg_steps = sum(total_steps) / num_episodes
        success_rate = success_count / num_episodes * 100

        print(f"í‰ê·  ë³´ìƒ: {avg_reward:.2f}")
        print(f"í‰ê·  ìŠ¤í…: {avg_steps:.1f}")
        print(f"ì„±ê³µë¥ : {success_rate:.1f}%")
        print("=" * 60)

        return {
            "success_rate": success_rate,
            "avg_reward": avg_reward,
            "avg_steps": avg_steps
        }

    def _check_stage_progression(self, episode):
        """
        í˜„ì¬ ë‹¨ê³„ì—ì„œ ë‹¤ìŒ ë‹¨ê³„ë¡œ ì§„í–‰í•  ì¤€ë¹„ê°€ ë˜ì—ˆëŠ”ì§€ í™•ì¸

        Args:
            episode: í˜„ì¬ ì—í”¼ì†Œë“œ ë²ˆí˜¸

        Returns:
            bool: ë‹¤ìŒ ë‹¨ê³„ë¡œ ì§„í–‰ ê°€ëŠ¥ ì—¬ë¶€
        """
        # ì»¤ë¦¬í˜ëŸ¼ í•™ìŠµì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ê±°ë‚˜ ë§ˆì§€ë§‰ ë‹¨ê³„ì¸ ê²½ìš°
        if not self.curriculum_stages or self.current_stage_idx >= len(self.curriculum_stages) - 1:
            return False

        # ì¶©ë¶„í•œ ì—í”¼ì†Œë“œê°€ ì§„í–‰ë˜ì—ˆëŠ”ì§€ í™•ì¸ (ìµœì†Œ window í¬ê¸°ë§Œí¼)
        stage_episodes = episode - self.stage_start_episode
        if stage_episodes < self.progression_window:
            return False

        # ìµœê·¼ progression_window ì—í”¼ì†Œë“œì˜ ì„±ê³µë¥  ê³„ì‚°
        recent_results = self.stats["episode_results"][-self.progression_window:]

        # ì„±ê³µ ë° ë¶€ë¶„ ì„±ê³µ ì¹´ìš´íŠ¸
        success_count = sum(1 for r in recent_results if r == STATUS_WIN)
        partial_success_count = sum(1 for r in recent_results if r == STATUS_PARTIAL_WIN)

        # ì„±ê³µë¥  ê³„ì‚° (ë¶€ë¶„ ì„±ê³µì€ 0.5ë¡œ ê³„ì‚°)
        combined_success_rate = (success_count + partial_success_count * 0.5) / len(recent_results)

        if combined_success_rate >= self.progression_threshold:
            return True

        return False

    def _progress_to_next_stage(self, episode):
        """
        ë‹¤ìŒ ì»¤ë¦¬í˜ëŸ¼ ë‹¨ê³„ë¡œ ì§„í–‰

        Args:
            episode: í˜„ì¬ ì—í”¼ì†Œë“œ ë²ˆí˜¸
        """
        # í˜„ì¬ ë‹¨ê³„ ëª¨ë¸ ì €ì¥
        current_stage = self.curriculum_stages[self.current_stage_idx]
        stage_model_path = self.model_path.replace(".pth", f"_{current_stage['name']}.pth")
        os.makedirs(os.path.dirname(stage_model_path) if os.path.dirname(stage_model_path) else "outputs", exist_ok=True)
        self.agent.save(stage_model_path)

        print("\n" + "=" * 60)
        print(f"ğŸ“ ì»¤ë¦¬í˜ëŸ¼ ì§„í–‰: {current_stage['name']} ì™„ë£Œ!")
        print(f"   ì„±ê³µë¥ : {self.stats['success_count'] / len(self.stats['episode_rewards']) * 100:.1f}%")
        print(f"   ëª¨ë¸ ì €ì¥: {stage_model_path}")

        # ë‹¤ìŒ ë‹¨ê³„ë¡œ ì´ë™
        self.current_stage_idx += 1
        next_stage = self.curriculum_stages[self.current_stage_idx]
        self.stage_start_episode = episode

        # config ì—…ë°ì´íŠ¸
        import config
        config.NUM_ROBOTS = next_stage["num_robots"]
        config.MIN_ROBOT_DISTANCE = next_stage["min_distance"]

        # í†µê³„ ë¦¬ì…‹ (ì„ íƒì )
        # self.stats = {...}  # ë¦¬ì…‹í•˜ì§€ ì•Šê³  ëˆ„ì 

        print(f"ğŸš€ ë‹¤ìŒ ë‹¨ê³„ ì‹œì‘: {next_stage['name']}")
        print(f"   ë¡œë´‡ ìˆ˜: {next_stage['num_robots']}")
        print(f"   ìµœì†Œ ê±°ë¦¬: {next_stage['min_distance']}")
        print("=" * 60 + "\n")

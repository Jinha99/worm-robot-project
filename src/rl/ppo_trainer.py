"""
PPO Trainer for Worm Robot

Trajectory ìˆ˜ì§‘ ë° PPO ì—…ë°ì´íŠ¸ ê´€ë¦¬
"""

import os
from pypdevs.simulator import Simulator

from config import STATUS_WIN, STATUS_PARTIAL_WIN, STATUS_FAIL, STATUS_RUNNING

# TensorBoard ì§€ì› (ì„ íƒ)
try:
    from torch.utils.tensorboard import SummaryWriter
    TENSORBOARD_AVAILABLE = True
except ImportError:
    TENSORBOARD_AVAILABLE = False
    print("âš ï¸  TensorBoardë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ ì„¤ì¹˜í•˜ì„¸ìš”: pip3 install tensorboard")


class PPOTrainer:
    """
    PPO í•™ìŠµ ë£¨í”„ë¥¼ ê´€ë¦¬í•˜ëŠ” í´ë˜ìŠ¤

    ì—í”¼ì†Œë“œ ë‹¨ìœ„ë¡œ trajectory ìˆ˜ì§‘ í›„ PPO ì—…ë°ì´íŠ¸
    ì»¤ë¦¬í˜ëŸ¼ í•™ìŠµ ì§€ì›
    """

    def __init__(
        self,
        agent,
        create_system_fn,
        num_episodes=1000,
        termination_time=100,
        update_epochs=4,
        batch_size=64,
        log_interval=10,
        save_interval=100,
        model_path="models/ppo_worm_robot.pth",
        use_tensorboard=True,
        tensorboard_dir="runs/worm_robot_ppo",
        curriculum_stages=None,
        progression_threshold=0.7,
        progression_window=100
    ):
        """
        Args:
            agent: PPO ì—ì´ì „íŠ¸
            create_system_fn: WormRobotSystemì„ ìƒì„±í•˜ëŠ” í•¨ìˆ˜
            num_episodes: í•™ìŠµ ì—í”¼ì†Œë“œ ìˆ˜
            termination_time: ì‹œë®¬ë ˆì´ì…˜ ìµœëŒ€ ì‹œê°„ (ì´ˆ)
            update_epochs: PPO ì—…ë°ì´íŠ¸ ì—í­ ìˆ˜
            batch_size: ë¯¸ë‹ˆë°°ì¹˜ í¬ê¸°
            log_interval: ë¡œê·¸ ì¶œë ¥ ê°„ê²©
            save_interval: ëª¨ë¸ ì €ì¥ ê°„ê²©
            model_path: ëª¨ë¸ ì €ì¥ ê²½ë¡œ
            use_tensorboard: TensorBoard ì‚¬ìš© ì—¬ë¶€
            tensorboard_dir: TensorBoard ë¡œê·¸ ë””ë ‰í† ë¦¬
            curriculum_stages: ì»¤ë¦¬í˜ëŸ¼ í•™ìŠµ ë‹¨ê³„ ë¦¬ìŠ¤íŠ¸
            progression_threshold: ë‹¤ìŒ ë‹¨ê³„ë¡œ ì§„í–‰í•˜ê¸° ìœ„í•œ ì„±ê³µë¥  ì„ê³„ê°’
            progression_window: ì„±ê³µë¥  ê³„ì‚°ì— ì‚¬ìš©í•  ìµœê·¼ ì—í”¼ì†Œë“œ ìˆ˜
        """
        self.agent = agent
        self.create_system_fn = create_system_fn
        self.num_episodes = num_episodes
        self.termination_time = termination_time
        self.update_epochs = update_epochs
        self.batch_size = batch_size
        self.log_interval = log_interval
        self.save_interval = save_interval
        self.model_path = model_path

        # ì»¤ë¦¬í˜ëŸ¼ í•™ìŠµ ì„¤ì •
        self.curriculum_stages = curriculum_stages
        self.progression_threshold = progression_threshold
        self.progression_window = progression_window
        self.current_stage_idx = 0
        self.stage_start_episode = 0

        # TensorBoard
        self.writer = None
        if use_tensorboard and TENSORBOARD_AVAILABLE:
            self.writer = SummaryWriter(tensorboard_dir)
            print(f"ğŸ“Š TensorBoard ë¡œê¹… í™œì„±í™”: {tensorboard_dir}")
            print(f"   ì‹¤í–‰: tensorboard --logdir=runs")

        # í†µê³„
        self.stats = {
            "episode_rewards": [],
            "episode_steps": [],
            "episode_losses": [],
            "episode_results": [],  # ê° ì—í”¼ì†Œë“œ ê²°ê³¼
            "success_count": 0,
            "partial_success_count": 0,
            "fail_count": 0,
            "timeout_count": 0
        }

    def train(self):
        """í•™ìŠµ ë£¨í”„ ì‹¤í–‰ (ì»¤ë¦¬í˜ëŸ¼ í•™ìŠµ ì§€ì›)"""
        print("=" * 60)
        print("PPO í•™ìŠµ ì‹œì‘")
        print("=" * 60)
        print(f"ì—í”¼ì†Œë“œ ìˆ˜: {self.num_episodes}")
        print(f"ì‹œë®¬ë ˆì´ì…˜ ì‹œê°„: {self.termination_time}ì´ˆ")
        print(f"PPO ì—…ë°ì´íŠ¸ ì—í­: {self.update_epochs}")
        print(f"ë°°ì¹˜ í¬ê¸°: {self.batch_size}")

        # ì»¤ë¦¬í˜ëŸ¼ í•™ìŠµ ì •ë³´ ì¶œë ¥
        if self.curriculum_stages:
            print(f"\nğŸ“š ì»¤ë¦¬í˜ëŸ¼ í•™ìŠµ í™œì„±í™”:")
            for i, stage in enumerate(self.curriculum_stages):
                print(f"   {i+1}. {stage['name']}: {stage['num_robots']}ê°œ ë¡œë´‡, ìµœì†Œê±°ë¦¬ {stage['min_distance']}")
            print(f"   ì§„í–‰ ì¡°ê±´: ì„±ê³µë¥  {self.progression_threshold*100:.0f}% (ìµœê·¼ {self.progression_window} ì—í”¼ì†Œë“œ)")

            # ì²« ë²ˆì§¸ ë‹¨ê³„ ì„¤ì •
            import config
            first_stage = self.curriculum_stages[0]
            config.NUM_ROBOTS = first_stage["num_robots"]
            config.MIN_ROBOT_DISTANCE = first_stage["min_distance"]
            print(f"\nğŸš€ ì‹œì‘ ë‹¨ê³„: {first_stage['name']}")

        print("=" * 60)

        for episode in range(self.num_episodes):
            # ì—í”¼ì†Œë“œ ì‹¤í–‰ ë° trajectory ìˆ˜ì§‘
            episode_reward, episode_steps, episode_status, trajectory = self._run_episode()

            # í†µê³„ ì—…ë°ì´íŠ¸
            self.stats["episode_rewards"].append(episode_reward)
            self.stats["episode_steps"].append(episode_steps)
            self.stats["episode_results"].append(episode_status)

            if episode_status == STATUS_WIN:
                self.stats["success_count"] += 1
            elif episode_status == STATUS_PARTIAL_WIN:
                self.stats["partial_success_count"] += 1
            elif episode_status == STATUS_FAIL:
                self.stats["fail_count"] += 1
            else:  # STATUS_RUNNING (ì‹œê°„ ì´ˆê³¼)
                self.stats["timeout_count"] += 1

            # PPO ì—…ë°ì´íŠ¸
            if len(trajectory['states']) > 0:
                loss = self._update_policy(trajectory)
                self.stats["episode_losses"].append(loss)
            else:
                self.stats["episode_losses"].append(0.0)

            # ì»¤ë¦¬í˜ëŸ¼ ë‹¨ê³„ ì§„í–‰ ì²´í¬
            if self._check_stage_progression(episode):
                self._progress_to_next_stage(episode)

            # TensorBoard ë¡œê¹…
            if self.writer is not None:
                self.writer.add_scalar('Reward/episode', episode_reward, episode)
                self.writer.add_scalar('Steps/episode', episode_steps, episode)
                self.writer.add_scalar('Loss/episode', self.stats["episode_losses"][-1], episode)
                self.writer.add_scalar('Success/total', self.stats["success_count"], episode)
                self.writer.add_scalar('Success/partial', self.stats["partial_success_count"], episode)
                self.writer.add_scalar('Fail/total', self.stats["fail_count"], episode)
                self.writer.add_scalar('Timeout/total', self.stats["timeout_count"], episode)

                if episode_status == STATUS_WIN:
                    self.writer.add_scalar('Result/win', 1, episode)
                    self.writer.add_scalar('Result/partial', 0, episode)
                    self.writer.add_scalar('Result/fail', 0, episode)
                    self.writer.add_scalar('Result/timeout', 0, episode)
                elif episode_status == STATUS_PARTIAL_WIN:
                    self.writer.add_scalar('Result/win', 0, episode)
                    self.writer.add_scalar('Result/partial', 1, episode)
                    self.writer.add_scalar('Result/fail', 0, episode)
                    self.writer.add_scalar('Result/timeout', 0, episode)
                elif episode_status == STATUS_FAIL:
                    self.writer.add_scalar('Result/win', 0, episode)
                    self.writer.add_scalar('Result/partial', 0, episode)
                    self.writer.add_scalar('Result/fail', 1, episode)
                    self.writer.add_scalar('Result/timeout', 0, episode)
                else:
                    self.writer.add_scalar('Result/win', 0, episode)
                    self.writer.add_scalar('Result/partial', 0, episode)
                    self.writer.add_scalar('Result/fail', 0, episode)
                    self.writer.add_scalar('Result/timeout', 1, episode)

            # ë¡œê·¸ ì¶œë ¥
            if (episode + 1) % self.log_interval == 0:
                recent = self.log_interval
                avg_reward = sum(self.stats["episode_rewards"][-recent:]) / recent
                avg_steps = sum(self.stats["episode_steps"][-recent:]) / recent
                avg_loss = sum(self.stats["episode_losses"][-recent:]) / recent

                print(
                    f"Ep {episode + 1:4d}/{self.num_episodes} | "
                    f"Reward: {avg_reward:6.1f} | "
                    f"Steps: {avg_steps:4.1f} | "
                    f"Loss: {avg_loss:.4f} | "
                    f"W: {self.stats['success_count']:3d} | "
                    f"P: {self.stats['partial_success_count']:3d} | "
                    f"F: {self.stats['fail_count']:3d} | "
                    f"T: {self.stats['timeout_count']:3d}"
                )

            # ëª¨ë¸ ì €ì¥
            if (episode + 1) % self.save_interval == 0:
                self._save_model()

        # ìµœì¢… ëª¨ë¸ ì €ì¥
        self._save_model()

        # TensorBoard writer ì¢…ë£Œ
        if self.writer is not None:
            self.writer.close()
            print("\nğŸ“Š TensorBoard ë¡œê·¸ ì €ì¥ ì™„ë£Œ")

        print("\n" + "=" * 60)
        print("í•™ìŠµ ì™„ë£Œ!")
        print(f"ì´ ì™„ì „ ì„±ê³µ: {self.stats['success_count']} ({self.stats['success_count'] / self.num_episodes * 100:.1f}%)")
        print(f"ì´ ë¶€ë¶„ ì„±ê³µ: {self.stats['partial_success_count']} ({self.stats['partial_success_count'] / self.num_episodes * 100:.1f}%)")
        print(f"ì´ ì¶©ëŒ ì‹¤íŒ¨: {self.stats['fail_count']} ({self.stats['fail_count'] / self.num_episodes * 100:.1f}%)")
        print(f"ì´ ì‹œê°„ ì´ˆê³¼: {self.stats['timeout_count']} ({self.stats['timeout_count'] / self.num_episodes * 100:.1f}%)")
        combined_success = self.stats['success_count'] + self.stats['partial_success_count']
        print(f"ì „ì²´ ì„±ê³µë¥ : {combined_success / self.num_episodes * 100:.1f}%")
        print("=" * 60)

        return self.stats

    def _run_episode(self):
        """
        ë‹¨ì¼ ì—í”¼ì†Œë“œ ì‹¤í–‰ ë° trajectory ìˆ˜ì§‘

        Returns:
            tuple: (total_reward, step_count, final_status, trajectory)
        """
        # ìƒˆë¡œìš´ ì‹œìŠ¤í…œ ìƒì„±
        system = self.create_system_fn(rl_agent=self.agent)

        # trajectory ì €ì¥ìš©
        trajectory = {
            'states': [],
            'actions': [],
            'log_probs': [],
            'values': [],
            'rewards': [],
            'dones': []
        }

        # ì‹œë®¬ë ˆì´í„° ì‹¤í–‰
        sim = Simulator(system)
        sim.setClassicDEVS()
        sim.setTerminationTime(self.termination_time)
        sim.simulate()

        # ìµœì¢… ìƒíƒœ ìˆ˜ì§‘
        final_status = system.environment.state.status
        step_count = system.environment.state.step_count

        # Controllerì—ì„œ PPO trajectory ìˆ˜ì§‘
        controller = system.controller
        trajectory = controller.get_ppo_trajectory()

        # ì´ ë³´ìƒ ê³„ì‚°
        total_reward = sum(trajectory['rewards']) if trajectory['rewards'] else 0.0

        return total_reward, step_count, final_status, trajectory

    def _update_policy(self, trajectory):
        """
        PPO ì •ì±… ì—…ë°ì´íŠ¸

        Args:
            trajectory: ì—í”¼ì†Œë“œ trajectory

        Returns:
            float: í‰ê·  ì†ì‹¤
        """
        if len(trajectory['states']) == 0:
            return 0.0

        import torch

        states = trajectory['states']
        actions = trajectory['actions']
        rewards = trajectory['rewards']
        dones = trajectory['dones']
        old_log_probs = trajectory['log_probs']
        values = trajectory['values']

        # ë§ˆì§€ë§‰ ìƒíƒœì˜ ê°€ì¹˜ ê³„ì‚° (ë¶€íŠ¸ìŠ¤íŠ¸ë˜í•‘ìš©)
        if len(states) > 0:
            import numpy as np
            last_state = torch.FloatTensor(np.array(states[-1])).unsqueeze(0).to(self.agent.device)
            with torch.no_grad():
                next_value = self.agent.critic(last_state).item()
        else:
            next_value = 0.0

        # GAE ê³„ì‚°
        advantages, returns = self.agent.compute_gae(rewards, values, dones, next_value)

        # statesë¥¼ numpy arrayë¡œ ë³€í™˜ (ê²½ê³  ë°©ì§€)
        import numpy as np
        states_array = np.array(states, dtype=np.float32)

        # PPO ì—…ë°ì´íŠ¸
        loss = self.agent.update(
            states=states_array,
            actions=actions,
            old_log_probs=old_log_probs,
            returns=returns,
            advantages=advantages,
            epochs=self.update_epochs,
            batch_size=self.batch_size
        )

        return loss

    def _save_model(self):
        """ëª¨ë¸ ì €ì¥"""
        os.makedirs(os.path.dirname(self.model_path) if os.path.dirname(self.model_path) else "models", exist_ok=True)
        self.agent.save(self.model_path)

    def _check_stage_progression(self, episode):
        """í˜„ì¬ ë‹¨ê³„ì—ì„œ ë‹¤ìŒ ë‹¨ê³„ë¡œ ì§„í–‰í•  ì¤€ë¹„ê°€ ë˜ì—ˆëŠ”ì§€ í™•ì¸"""
        if not self.curriculum_stages or self.current_stage_idx >= len(self.curriculum_stages) - 1:
            return False

        stage_episodes = episode - self.stage_start_episode
        if stage_episodes < self.progression_window:
            return False

        recent_results = self.stats["episode_results"][-self.progression_window:]
        success_count = sum(1 for r in recent_results if r == STATUS_WIN)
        partial_success_count = sum(1 for r in recent_results if r == STATUS_PARTIAL_WIN)
        combined_success_rate = (success_count + partial_success_count * 0.5) / len(recent_results)

        if combined_success_rate >= self.progression_threshold:
            return True

        return False

    def _progress_to_next_stage(self, episode):
        """ë‹¤ìŒ ì»¤ë¦¬í˜ëŸ¼ ë‹¨ê³„ë¡œ ì§„í–‰"""
        current_stage = self.curriculum_stages[self.current_stage_idx]
        stage_model_path = self.model_path.replace(".pth", f"_{current_stage['name']}.pth")
        os.makedirs(os.path.dirname(stage_model_path) if os.path.dirname(stage_model_path) else "outputs", exist_ok=True)
        self.agent.save(stage_model_path)

        print("\n" + "=" * 60)
        print(f"ğŸ“ ì»¤ë¦¬í˜ëŸ¼ ì§„í–‰: {current_stage['name']} ì™„ë£Œ!")
        print(f"   ì„±ê³µë¥ : {self.stats['success_count'] / len(self.stats['episode_rewards']) * 100:.1f}%")
        print(f"   ëª¨ë¸ ì €ì¥: {stage_model_path}")

        self.current_stage_idx += 1
        next_stage = self.curriculum_stages[self.current_stage_idx]
        self.stage_start_episode = episode

        import config
        config.NUM_ROBOTS = next_stage["num_robots"]
        config.MIN_ROBOT_DISTANCE = next_stage["min_distance"]

        print(f"ğŸš€ ë‹¤ìŒ ë‹¨ê³„ ì‹œì‘: {next_stage['name']}")
        print(f"   ë¡œë´‡ ìˆ˜: {next_stage['num_robots']}")
        print(f"   ìµœì†Œ ê±°ë¦¬: {next_stage['min_distance']}")
        print("=" * 60 + "\n")

"""
PPO (Proximal Policy Optimization) Agent for Worm Robot

Actor-Critic êµ¬ì¡°:
- Actor: ì •ì±… ë„¤íŠ¸ì›Œí¬ Ï€(a|s) - í–‰ë™ í™•ë¥  ë¶„í¬ ì¶œë ¥
- Critic: ê°€ì¹˜ ë„¤íŠ¸ì›Œí¬ V(s) - ìƒíƒœ ê°€ì¹˜ ì¶”ì •
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.distributions import Categorical
import numpy as np
import os


class ActorNetwork(nn.Module):
    """ì •ì±… ë„¤íŠ¸ì›Œí¬ (Actor)"""

    def __init__(self, state_dim, action_dim, hidden_dim=128):
        super(ActorNetwork, self).__init__()
        self.fc1 = nn.Linear(state_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.fc3 = nn.Linear(hidden_dim, action_dim)

    def forward(self, state):
        """
        Args:
            state: ìƒíƒœ ë²¡í„°

        Returns:
            action_probs: ê° í–‰ë™ì˜ í™•ë¥  ë¶„í¬
        """
        x = F.relu(self.fc1(state))
        x = F.relu(self.fc2(x))
        action_logits = self.fc3(x)
        action_probs = F.softmax(action_logits, dim=-1)
        return action_probs


class CriticNetwork(nn.Module):
    """ê°€ì¹˜ ë„¤íŠ¸ì›Œí¬ (Critic)"""

    def __init__(self, state_dim, hidden_dim=128):
        super(CriticNetwork, self).__init__()
        self.fc1 = nn.Linear(state_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.fc3 = nn.Linear(hidden_dim, 1)

    def forward(self, state):
        """
        Args:
            state: ìƒíƒœ ë²¡í„°

        Returns:
            value: ìƒíƒœ ê°€ì¹˜ V(s)
        """
        x = F.relu(self.fc1(state))
        x = F.relu(self.fc2(x))
        value = self.fc3(x)
        return value


class PPOAgent:
    """PPO ì—ì´ì „íŠ¸"""

    def __init__(
        self,
        state_dim,
        action_dim,
        learning_rate=3e-4,
        gamma=0.99,
        gae_lambda=0.95,
        clip_epsilon=0.2,
        value_coef=0.5,
        entropy_coef=0.01,
        max_grad_norm=0.5,
        hidden_dim=128,
        device="cpu"
    ):
        """
        Args:
            state_dim: ìƒíƒœ ì°¨ì›
            action_dim: í–‰ë™ ì°¨ì›
            learning_rate: í•™ìŠµë¥ 
            gamma: í• ì¸ìœ¨
            gae_lambda: GAE lambda íŒŒë¼ë¯¸í„°
            clip_epsilon: PPO clip ë²”ìœ„
            value_coef: ê°€ì¹˜ ì†ì‹¤ ê³„ìˆ˜
            entropy_coef: ì—”íŠ¸ë¡œí”¼ ë³´ë„ˆìŠ¤ ê³„ìˆ˜
            max_grad_norm: ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘ ìµœëŒ€ê°’
            hidden_dim: ì€ë‹‰ì¸µ ì°¨ì›
            device: ë””ë°”ì´ìŠ¤ (cpu/cuda)
        """
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.gamma = gamma
        self.gae_lambda = gae_lambda
        self.clip_epsilon = clip_epsilon
        self.value_coef = value_coef
        self.entropy_coef = entropy_coef
        self.max_grad_norm = max_grad_norm
        self.device = torch.device(device)

        # ë„¤íŠ¸ì›Œí¬ ì´ˆê¸°í™”
        self.actor = ActorNetwork(state_dim, action_dim, hidden_dim).to(self.device)
        self.critic = CriticNetwork(state_dim, hidden_dim).to(self.device)

        # ì˜µí‹°ë§ˆì´ì €
        self.optimizer = optim.Adam(
            list(self.actor.parameters()) + list(self.critic.parameters()),
            lr=learning_rate
        )

    def select_action(self, state, action_mask=None):
        """
        ì •ì±…ì— ë”°ë¼ í–‰ë™ ì„ íƒ

        Args:
            state: í˜„ì¬ ìƒíƒœ (numpy array)
            action_mask: í–‰ë™ ë§ˆìŠ¤í¬ (Noneì´ë©´ ëª¨ë“  í–‰ë™ í—ˆìš©)
                        [1.0, 0.5, 0.8] í˜•íƒœë¡œ ê° í–‰ë™ì˜ í—ˆìš© í™•ë¥ 

        Returns:
            action: ì„ íƒëœ í–‰ë™
            log_prob: í–‰ë™ì˜ ë¡œê·¸ í™•ë¥ 
            value: ìƒíƒœ ê°€ì¹˜
        """
        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)

        with torch.no_grad():
            action_probs = self.actor(state_tensor)
            value = self.critic(state_tensor)

            # í–‰ë™ ë§ˆìŠ¤í¬ ì ìš©
            if action_mask is not None:
                mask_tensor = torch.FloatTensor(action_mask).to(self.device)
                action_probs = action_probs * mask_tensor
                # ì¬ì •ê·œí™”
                action_probs = action_probs / action_probs.sum(dim=-1, keepdim=True)

            # í™•ë¥  ë¶„í¬ì—ì„œ ìƒ˜í”Œë§
            dist = Categorical(action_probs)
            action = dist.sample()
            log_prob = dist.log_prob(action)

        return action.item(), log_prob.item(), value.item()

    def evaluate_actions(self, states, actions):
        """
        ì£¼ì–´ì§„ ìƒíƒœ-í–‰ë™ ìŒì— ëŒ€í•œ í‰ê°€

        Args:
            states: ìƒíƒœ ë°°ì¹˜
            actions: í–‰ë™ ë°°ì¹˜

        Returns:
            log_probs: í–‰ë™ì˜ ë¡œê·¸ í™•ë¥ 
            values: ìƒíƒœ ê°€ì¹˜
            entropy: ì •ì±… ì—”íŠ¸ë¡œí”¼
        """
        action_probs = self.actor(states)
        values = self.critic(states)

        dist = Categorical(action_probs)
        log_probs = dist.log_prob(actions)
        entropy = dist.entropy()

        return log_probs, values.squeeze(-1), entropy

    def compute_gae(self, rewards, values, dones, next_value):
        """
        GAE (Generalized Advantage Estimation) ê³„ì‚°

        Args:
            rewards: ë³´ìƒ ë¦¬ìŠ¤íŠ¸
            values: ê°€ì¹˜ ë¦¬ìŠ¤íŠ¸
            dones: ì¢…ë£Œ í”Œë˜ê·¸ ë¦¬ìŠ¤íŠ¸
            next_value: ë‹¤ìŒ ìƒíƒœ ê°€ì¹˜

        Returns:
            advantages: ì–´ë“œë°´í‹°ì§€
            returns: ë¦¬í„´ (íƒ€ê²Ÿ ê°€ì¹˜)
        """
        advantages = []
        gae = 0

        # ì—­ìˆœìœ¼ë¡œ ê³„ì‚°
        for i in reversed(range(len(rewards))):
            if i == len(rewards) - 1:
                next_val = next_value
            else:
                next_val = values[i + 1]

            # TD error: Î´_t = r_t + Î³V(s_{t+1}) - V(s_t)
            delta = rewards[i] + self.gamma * next_val * (1 - dones[i]) - values[i]

            # GAE: A_t = Î´_t + (Î³Î»)Î´_{t+1} + (Î³Î»)^2Î´_{t+2} + ...
            gae = delta + self.gamma * self.gae_lambda * (1 - dones[i]) * gae
            advantages.insert(0, gae)

        advantages = torch.FloatTensor(advantages).to(self.device)
        returns = advantages + torch.FloatTensor(values).to(self.device)

        return advantages, returns

    def update(self, states, actions, old_log_probs, returns, advantages, epochs=4, batch_size=64):
        """
        PPO ì—…ë°ì´íŠ¸

        Args:
            states: ìƒíƒœ ë°°ì¹˜
            actions: í–‰ë™ ë°°ì¹˜
            old_log_probs: ì´ì „ ì •ì±…ì˜ ë¡œê·¸ í™•ë¥ 
            returns: ë¦¬í„´ (íƒ€ê²Ÿ ê°€ì¹˜)
            advantages: ì–´ë“œë°´í‹°ì§€
            epochs: ì—…ë°ì´íŠ¸ ì—í­ ìˆ˜
            batch_size: ë¯¸ë‹ˆë°°ì¹˜ í¬ê¸°

        Returns:
            mean_loss: í‰ê·  ì†ì‹¤
        """
        states = torch.FloatTensor(states).to(self.device)
        actions = torch.LongTensor(actions).to(self.device)
        old_log_probs = torch.FloatTensor(old_log_probs).to(self.device)
        returns = returns.to(self.device)
        advantages = advantages.to(self.device)

        # ì–´ë“œë°´í‹°ì§€ ì •ê·œí™” (stdê°€ 0ì´ ì•„ë‹ ë•Œë§Œ)
        adv_std = advantages.std()
        if adv_std > 1e-8:
            advantages = (advantages - advantages.mean()) / (adv_std + 1e-8)
        else:
            # ëª¨ë“  advantageê°€ ê°™ìœ¼ë©´ ì •ê·œí™”í•˜ì§€ ì•ŠìŒ
            advantages = advantages - advantages.mean()

        total_loss = 0
        num_updates = 0

        # ì—¬ëŸ¬ ì—í­ ë°˜ë³µ
        for _ in range(epochs):
            # ë¯¸ë‹ˆë°°ì¹˜ë¡œ ë‚˜ëˆ„ì–´ í•™ìŠµ
            for i in range(0, len(states), batch_size):
                batch_indices = slice(i, min(i + batch_size, len(states)))

                batch_states = states[batch_indices]
                batch_actions = actions[batch_indices]
                batch_old_log_probs = old_log_probs[batch_indices]
                batch_returns = returns[batch_indices]
                batch_advantages = advantages[batch_indices]

                # í˜„ì¬ ì •ì±… í‰ê°€
                log_probs, values, entropy = self.evaluate_actions(batch_states, batch_actions)

                # PPO clip ì†ì‹¤
                ratio = torch.exp(log_probs - batch_old_log_probs)
                surr1 = ratio * batch_advantages
                surr2 = torch.clamp(ratio, 1 - self.clip_epsilon, 1 + self.clip_epsilon) * batch_advantages
                policy_loss = -torch.min(surr1, surr2).mean()

                # ê°€ì¹˜ ì†ì‹¤
                value_loss = F.mse_loss(values, batch_returns)

                # ì—”íŠ¸ë¡œí”¼ ë³´ë„ˆìŠ¤ (íƒí—˜ ì¥ë ¤)
                entropy_loss = -entropy.mean()

                # ì „ì²´ ì†ì‹¤
                loss = policy_loss + self.value_coef * value_loss + self.entropy_coef * entropy_loss

                # ì—…ë°ì´íŠ¸
                self.optimizer.zero_grad()
                loss.backward()
                nn.utils.clip_grad_norm_(
                    list(self.actor.parameters()) + list(self.critic.parameters()),
                    self.max_grad_norm
                )
                self.optimizer.step()

                total_loss += loss.item()
                num_updates += 1

        mean_loss = total_loss / num_updates if num_updates > 0 else 0
        return mean_loss

    def save(self, path):
        """ëª¨ë¸ ì €ì¥"""
        os.makedirs(os.path.dirname(path) if os.path.dirname(path) else "models", exist_ok=True)
        torch.save({
            'actor_state_dict': self.actor.state_dict(),
            'critic_state_dict': self.critic.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
        }, path)
        print(f"ğŸ’¾ ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {path}")

    def load(self, path):
        """ëª¨ë¸ ë¡œë“œ"""
        checkpoint = torch.load(path, map_location=self.device)
        self.actor.load_state_dict(checkpoint['actor_state_dict'])
        self.critic.load_state_dict(checkpoint['critic_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        print(f"ğŸ“‚ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {path}")

"""
MAPPO (Multi-Agent PPO) Agent for Worm Robot

ì¤‘ì•™ì§‘ì¤‘ì‹ Critic (Centralized Critic):
- ëª¨ë“  ì—ì´ì „íŠ¸ì˜ ìƒíƒœë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ ê°€ì¹˜ í•¨ìˆ˜ í•™ìŠµ
- í•™ìŠµ ì‹œì—ë§Œ ì‚¬ìš©, ì‹¤í–‰ ì‹œì—ëŠ” ë¶ˆí•„ìš”

ë¶„ì‚°ì‹ Actor (Decentralized Actor):
- ê° ì—ì´ì „íŠ¸ëŠ” ìì‹ ì˜ ê´€ì°°ê°’ë§Œ ì‚¬ìš©í•˜ì—¬ í–‰ë™ ì„ íƒ
- ì‹¤í–‰ ì‹œ ì‚¬ìš©
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.distributions import Categorical
import numpy as np
import os


class DecentralizedActor(nn.Module):
    """ë¶„ì‚°ì‹ ì •ì±… ë„¤íŠ¸ì›Œí¬ (ê° ì—ì´ì „íŠ¸ì˜ ë¡œì»¬ ê´€ì°°ë§Œ ì‚¬ìš©)"""

    def __init__(self, state_dim, action_dim, hidden_dim=128):
        super(DecentralizedActor, self).__init__()
        self.fc1 = nn.Linear(state_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.fc3 = nn.Linear(hidden_dim, action_dim)

    def forward(self, state):
        """
        Args:
            state: ê°œë³„ ì—ì´ì „íŠ¸ì˜ ë¡œì»¬ ê´€ì°° (state_dim,)

        Returns:
            action_probs: ê° í–‰ë™ì˜ í™•ë¥  ë¶„í¬
        """
        x = F.relu(self.fc1(state))
        x = F.relu(self.fc2(x))
        action_logits = self.fc3(x)
        action_probs = F.softmax(action_logits, dim=-1)
        return action_probs


class CentralizedCritic(nn.Module):
    """ì¤‘ì•™ì§‘ì¤‘ì‹ ê°€ì¹˜ ë„¤íŠ¸ì›Œí¬ (ëª¨ë“  ì—ì´ì „íŠ¸ì˜ ìƒíƒœ ì‚¬ìš©)"""

    def __init__(self, global_state_dim, hidden_dim=128):
        super(CentralizedCritic, self).__init__()
        self.fc1 = nn.Linear(global_state_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.fc3 = nn.Linear(hidden_dim, 1)

    def forward(self, global_state):
        """
        Args:
            global_state: ëª¨ë“  ì—ì´ì „íŠ¸ì˜ ìƒíƒœë¥¼ í•©ì¹œ ì „ì—­ ìƒíƒœ

        Returns:
            value: ì „ì—­ ìƒíƒœ ê°€ì¹˜ V(s_global)
        """
        x = F.relu(self.fc1(global_state))
        x = F.relu(self.fc2(x))
        value = self.fc3(x)
        return value


class MAPPOAgent:
    """MAPPO ì—ì´ì „íŠ¸"""

    def __init__(
        self,
        state_dim,
        global_state_dim,
        action_dim,
        learning_rate=3e-4,
        gamma=0.99,
        gae_lambda=0.95,
        clip_epsilon=0.2,
        value_coef=0.5,
        entropy_coef=0.01,
        max_grad_norm=0.5,
        hidden_dim=128,
        device="cpu"
    ):
        """
        Args:
            state_dim: ê°œë³„ ì—ì´ì „íŠ¸ì˜ ë¡œì»¬ ìƒíƒœ ì°¨ì›
            global_state_dim: ì „ì—­ ìƒíƒœ ì°¨ì› (ëª¨ë“  ì—ì´ì „íŠ¸ ì •ë³´ í¬í•¨)
            action_dim: í–‰ë™ ì°¨ì›
            learning_rate: í•™ìŠµë¥ 
            gamma: í• ì¸ìœ¨
            gae_lambda: GAE lambda íŒŒë¼ë¯¸í„°
            clip_epsilon: PPO clip ë²”ìœ„
            value_coef: ê°€ì¹˜ ì†ì‹¤ ê³„ìˆ˜
            entropy_coef: ì—”íŠ¸ë¡œí”¼ ë³´ë„ˆìŠ¤ ê³„ìˆ˜
            max_grad_norm: ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘ ìµœëŒ€ê°’
            hidden_dim: ì€ë‹‰ì¸µ ì°¨ì›
            device: ë””ë°”ì´ìŠ¤ (cpu/cuda)
        """
        self.state_dim = state_dim
        self.global_state_dim = global_state_dim
        self.action_dim = action_dim
        self.gamma = gamma
        self.gae_lambda = gae_lambda
        self.clip_epsilon = clip_epsilon
        self.value_coef = value_coef
        self.entropy_coef = entropy_coef
        self.max_grad_norm = max_grad_norm
        self.device = torch.device(device)

        # ë„¤íŠ¸ì›Œí¬ ì´ˆê¸°í™”
        self.actor = DecentralizedActor(state_dim, action_dim, hidden_dim).to(self.device)
        self.critic = CentralizedCritic(global_state_dim, hidden_dim).to(self.device)

        # ì˜µí‹°ë§ˆì´ì €
        self.optimizer = optim.Adam(
            list(self.actor.parameters()) + list(self.critic.parameters()),
            lr=learning_rate
        )

    def select_action(self, state, global_state, action_mask=None):
        """
        ì •ì±…ì— ë”°ë¼ í–‰ë™ ì„ íƒ

        Args:
            state: í˜„ì¬ ë¡œì»¬ ìƒíƒœ (numpy array)
            global_state: í˜„ì¬ ì „ì—­ ìƒíƒœ (numpy array)
            action_mask: í–‰ë™ ë§ˆìŠ¤í¬ (Noneì´ë©´ ëª¨ë“  í–‰ë™ í—ˆìš©)

        Returns:
            action: ì„ íƒëœ í–‰ë™
            log_prob: í–‰ë™ì˜ ë¡œê·¸ í™•ë¥ 
            value: ì „ì—­ ìƒíƒœ ê°€ì¹˜
        """
        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)
        global_state_tensor = torch.FloatTensor(global_state).unsqueeze(0).to(self.device)

        with torch.no_grad():
            action_probs = self.actor(state_tensor)
            value = self.critic(global_state_tensor)

            # í–‰ë™ ë§ˆìŠ¤í¬ ì ìš©
            if action_mask is not None:
                mask_tensor = torch.FloatTensor(action_mask).to(self.device)
                action_probs = action_probs * mask_tensor
                # ì¬ì •ê·œí™”
                action_probs = action_probs / action_probs.sum(dim=-1, keepdim=True)

            # í™•ë¥  ë¶„í¬ì—ì„œ ìƒ˜í”Œë§
            dist = Categorical(action_probs)
            action = dist.sample()
            log_prob = dist.log_prob(action)

        return action.item(), log_prob.item(), value.item()

    def evaluate_actions(self, states, global_states, actions):
        """
        ì£¼ì–´ì§„ ìƒíƒœ-í–‰ë™ ìŒì— ëŒ€í•œ í‰ê°€

        Args:
            states: ë¡œì»¬ ìƒíƒœ ë°°ì¹˜
            global_states: ì „ì—­ ìƒíƒœ ë°°ì¹˜
            actions: í–‰ë™ ë°°ì¹˜

        Returns:
            log_probs: í–‰ë™ì˜ ë¡œê·¸ í™•ë¥ 
            values: ì „ì—­ ìƒíƒœ ê°€ì¹˜
            entropy: ì •ì±… ì—”íŠ¸ë¡œí”¼
        """
        action_probs = self.actor(states)
        values = self.critic(global_states)

        dist = Categorical(action_probs)
        log_probs = dist.log_prob(actions)
        entropy = dist.entropy()

        return log_probs, values.squeeze(-1), entropy

    def compute_gae(self, rewards, values, dones, next_value):
        """
        GAE (Generalized Advantage Estimation) ê³„ì‚°

        Args:
            rewards: ë³´ìƒ ë¦¬ìŠ¤íŠ¸
            values: ê°€ì¹˜ ë¦¬ìŠ¤íŠ¸
            dones: ì¢…ë£Œ í”Œë˜ê·¸ ë¦¬ìŠ¤íŠ¸
            next_value: ë‹¤ìŒ ìƒíƒœ ê°€ì¹˜

        Returns:
            advantages: ì–´ë“œë°´í‹°ì§€
            returns: ë¦¬í„´ (íƒ€ê²Ÿ ê°€ì¹˜)
        """
        advantages = []
        gae = 0

        # ì—­ìˆœìœ¼ë¡œ ê³„ì‚°
        for i in reversed(range(len(rewards))):
            if i == len(rewards) - 1:
                next_val = next_value
            else:
                next_val = values[i + 1]

            # TD error: Î´_t = r_t + Î³V(s_{t+1}) - V(s_t)
            delta = rewards[i] + self.gamma * next_val * (1 - dones[i]) - values[i]

            # GAE: A_t = Î´_t + (Î³Î»)Î´_{t+1} + (Î³Î»)^2Î´_{t+2} + ...
            gae = delta + self.gamma * self.gae_lambda * (1 - dones[i]) * gae
            advantages.insert(0, gae)

        advantages = torch.FloatTensor(advantages).to(self.device)
        returns = advantages + torch.FloatTensor(values).to(self.device)

        return advantages, returns

    def update(self, states, global_states, actions, old_log_probs, returns, advantages, epochs=4, batch_size=64):
        """
        MAPPO ì—…ë°ì´íŠ¸

        Args:
            states: ë¡œì»¬ ìƒíƒœ ë°°ì¹˜
            global_states: ì „ì—­ ìƒíƒœ ë°°ì¹˜
            actions: í–‰ë™ ë°°ì¹˜
            old_log_probs: ì´ì „ ì •ì±…ì˜ ë¡œê·¸ í™•ë¥ 
            returns: ë¦¬í„´ (íƒ€ê²Ÿ ê°€ì¹˜)
            advantages: ì–´ë“œë°´í‹°ì§€
            epochs: ì—…ë°ì´íŠ¸ ì—í­ ìˆ˜
            batch_size: ë¯¸ë‹ˆë°°ì¹˜ í¬ê¸°

        Returns:
            mean_loss: í‰ê·  ì†ì‹¤
        """
        states = torch.FloatTensor(states).to(self.device)
        global_states = torch.FloatTensor(global_states).to(self.device)
        actions = torch.LongTensor(actions).to(self.device)
        old_log_probs = torch.FloatTensor(old_log_probs).to(self.device)
        returns = returns.to(self.device)
        advantages = advantages.to(self.device)

        # ì–´ë“œë°´í‹°ì§€ ì •ê·œí™” (stdê°€ 0ì´ ì•„ë‹ ë•Œë§Œ)
        adv_std = advantages.std()
        if adv_std > 1e-8:
            advantages = (advantages - advantages.mean()) / (adv_std + 1e-8)
        else:
            advantages = advantages - advantages.mean()

        total_loss = 0
        num_updates = 0

        # ì—¬ëŸ¬ ì—í­ ë°˜ë³µ
        for _ in range(epochs):
            # ë¯¸ë‹ˆë°°ì¹˜ë¡œ ë‚˜ëˆ„ì–´ í•™ìŠµ
            for i in range(0, len(states), batch_size):
                batch_indices = slice(i, min(i + batch_size, len(states)))

                batch_states = states[batch_indices]
                batch_global_states = global_states[batch_indices]
                batch_actions = actions[batch_indices]
                batch_old_log_probs = old_log_probs[batch_indices]
                batch_returns = returns[batch_indices]
                batch_advantages = advantages[batch_indices]

                # í˜„ì¬ ì •ì±… í‰ê°€
                log_probs, values, entropy = self.evaluate_actions(
                    batch_states, batch_global_states, batch_actions
                )

                # PPO clip ì†ì‹¤
                ratio = torch.exp(log_probs - batch_old_log_probs)
                surr1 = ratio * batch_advantages
                surr2 = torch.clamp(ratio, 1 - self.clip_epsilon, 1 + self.clip_epsilon) * batch_advantages
                policy_loss = -torch.min(surr1, surr2).mean()

                # ê°€ì¹˜ ì†ì‹¤ (ì¤‘ì•™ì§‘ì¤‘ì‹ Critic ì‚¬ìš©)
                value_loss = F.mse_loss(values, batch_returns)

                # ì—”íŠ¸ë¡œí”¼ ë³´ë„ˆìŠ¤ (íƒí—˜ ì¥ë ¤)
                entropy_loss = -entropy.mean()

                # ì „ì²´ ì†ì‹¤
                loss = policy_loss + self.value_coef * value_loss + self.entropy_coef * entropy_loss

                # ì—…ë°ì´íŠ¸
                self.optimizer.zero_grad()
                loss.backward()
                nn.utils.clip_grad_norm_(
                    list(self.actor.parameters()) + list(self.critic.parameters()),
                    self.max_grad_norm
                )
                self.optimizer.step()

                total_loss += loss.item()
                num_updates += 1

        mean_loss = total_loss / num_updates if num_updates > 0 else 0
        return mean_loss

    def save(self, path):
        """ëª¨ë¸ ì €ì¥"""
        os.makedirs(os.path.dirname(path) if os.path.dirname(path) else "models", exist_ok=True)
        torch.save({
            'actor_state_dict': self.actor.state_dict(),
            'critic_state_dict': self.critic.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
        }, path)
        print(f"ğŸ’¾ ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {path}")

    def load(self, path):
        """ëª¨ë¸ ë¡œë“œ"""
        checkpoint = torch.load(path, map_location=self.device)
        self.actor.load_state_dict(checkpoint['actor_state_dict'])
        self.critic.load_state_dict(checkpoint['critic_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        print(f"ğŸ“‚ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {path}")
